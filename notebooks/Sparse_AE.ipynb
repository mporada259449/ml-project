{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>1.168375e-15</td>\n",
       "      <td>3.416908e-16</td>\n",
       "      <td>-1.379537e-15</td>\n",
       "      <td>2.074095e-15</td>\n",
       "      <td>9.604066e-16</td>\n",
       "      <td>1.487313e-15</td>\n",
       "      <td>-5.556467e-16</td>\n",
       "      <td>1.213481e-16</td>\n",
       "      <td>-2.406331e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.654067e-16</td>\n",
       "      <td>-3.568593e-16</td>\n",
       "      <td>2.578648e-16</td>\n",
       "      <td>4.473266e-15</td>\n",
       "      <td>5.340915e-16</td>\n",
       "      <td>1.683437e-15</td>\n",
       "      <td>-3.660091e-16</td>\n",
       "      <td>-1.227390e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  1.168375e-15  3.416908e-16 -1.379537e-15  2.074095e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   9.604066e-16  1.487313e-15 -5.556467e-16  1.213481e-16 -2.406331e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   ...  1.654067e-16 -3.568593e-16  2.578648e-16  4.473266e-15   \n",
       "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   5.340915e-16  1.683437e-15 -3.660091e-16 -1.227390e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/creditcard.csv')\n",
    "df = pd.DataFrame(data) \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frauds:  492 \n",
      "Numbers of no frauds:  284315\n"
     ]
    }
   ],
   "source": [
    "number_fraud = len(data[data.Class == 1])\n",
    "number_no_fraud = len(data[data.Class == 0])\n",
    "print('Number of frauds: ', number_fraud,'\\nNumbers of no frauds: ', number_no_fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Amount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1, 1))\n",
    "df_honest = df.query('Class == 0').sample(20000)\n",
    "df_fraud = df.query('Class == 1').sample(400)\n",
    "df = pd.concat([df_honest, df_fraud])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16320, 29) train samples\n",
      "(4080, 29) test samples\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df.drop(labels=['Time', 'Class'], axis = 1) , \n",
    "                                                    df['Class'], test_size=0.2, random_state=42)\n",
    "print(x_train.shape, 'train samples')\n",
    "print(x_test.shape, 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAE(torch.nn.Module):\n",
    "    def __init__(self, sparsity_target, sparsity_weight) -> None:\n",
    "        super(SparseAE, self).__init__()\n",
    "        #self.encoder = torch.nn.Sequential(\n",
    "        #    torch.nn.Linear(29, 14),\n",
    "        #    torch.nn.LeakyReLU(),\n",
    "        #    torch.nn.Linear(14, 7),\n",
    "        #    torch.nn.LeakyReLU()\n",
    "        #    )\n",
    "        #self.decoder = torch.nn.Sequential(\n",
    "        #    torch.nn.Linear(7, 14),\n",
    "        #    torch.nn.LeakyReLU(),\n",
    "        #    torch.nn.Linear(14, 29),\n",
    "        #    torch.nn.LeakyReLU()\n",
    "        #)\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(29,12),\n",
    "            torch.nn.LeakyReLU()\n",
    "        )\n",
    "        self.decoder= torch.nn.Sequential(\n",
    "            torch.nn.Linear(12,29),\n",
    "            torch.nn.LeakyReLU()\n",
    "        )\n",
    "        self.sparsity_target = sparsity_target\n",
    "        self.sparsity_weight = sparsity_weight\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        self.data_rho = encoded.mean(0)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    def kl_divergence(self, rho_hat):\n",
    "        rho = self.sparsity_target\n",
    "        return rho * torch.log(rho / rho_hat) + (1 - rho) * torch.log((1 - rho) / (1 - rho_hat))\n",
    "\n",
    "    def loss_function(self, output, target, rho_hat):\n",
    "        reconstruction_loss = torch.nn.functional.mse_loss(output, target, reduction='mean')\n",
    "        sparsity_loss = self.sparsity_weight * torch.sum(self.kl_divergence(rho_hat))\n",
    "        return reconstruction_loss + sparsity_loss\n",
    "\n",
    "    \n",
    "sparsity_target = 0.5\n",
    "sparsity_weight = 0.5\n",
    "model = SparseAE(sparsity_target=sparsity_target, sparsity_weight=sparsity_weight).cpu()\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "lr = 1e-3\n",
    "#loss_function = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.tensor(x_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "x_test_tensor = torch.tensor(x_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = torch.utils.data.TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.8073\n",
      "Epoch [2/100], Loss: nan\n",
      "Epoch [3/100], Loss: nan\n",
      "Epoch [4/100], Loss: 1.3184\n",
      "Epoch [5/100], Loss: 1.2777\n",
      "Epoch [6/100], Loss: 1.2397\n",
      "Epoch [7/100], Loss: 1.2106\n",
      "Epoch [8/100], Loss: nan\n",
      "Epoch [9/100], Loss: 1.1738\n",
      "Epoch [10/100], Loss: 1.1582\n",
      "Epoch [11/100], Loss: 1.1429\n",
      "Epoch [12/100], Loss: 1.1336\n",
      "Epoch [13/100], Loss: 1.1228\n",
      "Epoch [14/100], Loss: 1.1156\n",
      "Epoch [15/100], Loss: 1.1118\n",
      "Epoch [16/100], Loss: 1.1059\n",
      "Epoch [17/100], Loss: 1.0980\n",
      "Epoch [18/100], Loss: 1.0941\n",
      "Epoch [19/100], Loss: 1.0915\n",
      "Epoch [20/100], Loss: 1.0869\n",
      "Epoch [21/100], Loss: 1.0838\n",
      "Epoch [22/100], Loss: 1.0800\n",
      "Epoch [23/100], Loss: 1.0781\n",
      "Epoch [24/100], Loss: 1.0769\n",
      "Epoch [25/100], Loss: 1.0749\n",
      "Epoch [26/100], Loss: 1.0700\n",
      "Epoch [27/100], Loss: 1.0703\n",
      "Epoch [28/100], Loss: 1.0692\n",
      "Epoch [29/100], Loss: 1.0668\n",
      "Epoch [30/100], Loss: 1.0650\n",
      "Epoch [31/100], Loss: 1.0634\n",
      "Epoch [32/100], Loss: 1.0630\n",
      "Epoch [33/100], Loss: 1.0599\n",
      "Epoch [34/100], Loss: 1.0604\n",
      "Epoch [35/100], Loss: 1.0590\n",
      "Epoch [36/100], Loss: 1.0596\n",
      "Epoch [37/100], Loss: 1.0573\n",
      "Epoch [38/100], Loss: 1.0563\n",
      "Epoch [39/100], Loss: 1.0557\n",
      "Epoch [40/100], Loss: 1.0543\n",
      "Epoch [41/100], Loss: 1.0535\n",
      "Epoch [42/100], Loss: 1.0526\n",
      "Epoch [43/100], Loss: 1.0513\n",
      "Epoch [44/100], Loss: 1.0514\n",
      "Epoch [45/100], Loss: 1.0505\n",
      "Epoch [46/100], Loss: 1.0510\n",
      "Epoch [47/100], Loss: 1.0490\n",
      "Epoch [48/100], Loss: 1.0483\n",
      "Epoch [49/100], Loss: 1.0477\n",
      "Epoch [50/100], Loss: 1.0490\n",
      "Epoch [51/100], Loss: 1.0453\n",
      "Epoch [52/100], Loss: 1.0428\n",
      "Epoch [53/100], Loss: 1.0439\n",
      "Epoch [54/100], Loss: 1.0432\n",
      "Epoch [55/100], Loss: 1.0389\n",
      "Epoch [56/100], Loss: 1.0393\n",
      "Epoch [57/100], Loss: 1.0366\n",
      "Epoch [58/100], Loss: 1.0353\n",
      "Epoch [59/100], Loss: 1.0341\n",
      "Epoch [60/100], Loss: 1.0323\n",
      "Epoch [61/100], Loss: 1.0318\n",
      "Epoch [62/100], Loss: 1.0284\n",
      "Epoch [63/100], Loss: 1.0306\n",
      "Epoch [64/100], Loss: 1.0274\n",
      "Epoch [65/100], Loss: 1.0282\n",
      "Epoch [66/100], Loss: 1.0268\n",
      "Epoch [67/100], Loss: 1.0274\n",
      "Epoch [68/100], Loss: 1.0339\n",
      "Epoch [69/100], Loss: 1.0239\n",
      "Epoch [70/100], Loss: 1.0239\n",
      "Epoch [71/100], Loss: 1.0241\n",
      "Epoch [72/100], Loss: nan\n",
      "Epoch [73/100], Loss: 1.0229\n",
      "Epoch [74/100], Loss: 1.0237\n",
      "Epoch [75/100], Loss: 1.0201\n",
      "Epoch [76/100], Loss: 1.0213\n",
      "Epoch [77/100], Loss: 1.0215\n",
      "Epoch [78/100], Loss: 1.0202\n",
      "Epoch [79/100], Loss: 1.0194\n",
      "Epoch [80/100], Loss: 1.0191\n",
      "Epoch [81/100], Loss: 1.0199\n",
      "Epoch [82/100], Loss: 1.0164\n",
      "Epoch [83/100], Loss: 1.0202\n",
      "Epoch [84/100], Loss: 1.0173\n",
      "Epoch [85/100], Loss: 1.0191\n",
      "Epoch [86/100], Loss: 1.0162\n",
      "Epoch [87/100], Loss: 1.0168\n",
      "Epoch [88/100], Loss: 1.0168\n",
      "Epoch [89/100], Loss: 1.0159\n",
      "Epoch [90/100], Loss: 1.0163\n",
      "Epoch [91/100], Loss: 1.0136\n",
      "Epoch [92/100], Loss: 1.0127\n",
      "Epoch [93/100], Loss: 1.0135\n",
      "Epoch [94/100], Loss: 1.0129\n",
      "Epoch [95/100], Loss: 1.0119\n",
      "Epoch [96/100], Loss: 1.0112\n",
      "Epoch [97/100], Loss: 1.0119\n",
      "Epoch [98/100], Loss: 1.0094\n",
      "Epoch [99/100], Loss: 1.0111\n",
      "Epoch [100/100], Loss: 1.0124\n"
     ]
    }
   ],
   "source": [
    "history = {}\n",
    "history['train_loss'] = []\n",
    "history['test_loss'] = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    h = np.array([])\n",
    "    for data_epoch, _ in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data_epoch)\n",
    "        rho_hat = torch.mean(model.encoder(data_epoch), dim=0)\n",
    "        loss = model.loss_function(output, data_epoch, rho_hat)\n",
    "        h = np.append(h, loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #running_loss += loss.item()\n",
    "     \n",
    "    epoch_loss = np.mean(h)\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}\")\n",
    "    history['train_loss'].append(epoch_loss)\n",
    "\n",
    "torch.save(model.state_dict, './fraud_model_sparse_ae.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
